{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lesson: extreme value distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Last week you learned *a lot* of new tools and concepts. You learned how to open a csv file, deal with data gaps in timeseries, and how to work with pandas. The lesson ended with the concept of **empirical return periods**, which is  an estimate of the average time interval between occurrences of an event (e.g., extreme rainfall, heatwave) based on observed data. Today we are going to discuss the limitations of empirical return periods, and propose an alternative approach.\n",
    "\n",
    "*Copyright notice: substantial parts of this class are inpired by the excellent [climatematch tutorials](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Tutorial2.html). I really recommend you to check them out!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Let's start by importing the packages as usual. We'll continue to use pandas today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # This is new\n",
    "from scipy import stats  # This also"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Revisiting the 1968 Bristol flood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "After last week's assignment, the catchment daily rainfall data (`53004_cdr.csv`) should already be in your `data/csv` folder. As a reminder, these are precipitation data for the Chew river near Bristol.\n",
    "\n",
    "We'll resume our work on the catchment daily rainfall timeseries. Let me do all the reading again for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/csv/53004_cdr.csv', skiprows=19, index_col=0, parse_dates=True)\n",
    "df.index.name = 'Date'\n",
    "df.columns = ['cdr', 'qual']\n",
    "\n",
    "# Check that time series is complete\n",
    "assert len(df) == len(pd.date_range(df.index[0], df.index[-1], freq='D')) \n",
    "assert df['cdr'].isnull().sum() == 0\n",
    "\n",
    "# Extract cdr\n",
    "cdr = df['cdr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Generalise the compute of empirical return periods with a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "As explained in the previous lesson, the computation of return periods involves ranking the data and then compute return periods based on these ranks (an event of rank 1 has a low probability of exceedance and a long return period, while an event ranked last has a high probability of exceedance and a short return period). These probabilities and ranks can be computed on data of any size and any type, therefore it can be generalised with a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_period(sample):\n",
    "    \"\"\"Compute the empirical return periods of a sample of extreme events.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : pandas.Series\n",
    "        A pandas Series containing observed extreme event magnitudes (e.g., annual maximum\n",
    "        precipitation, flood peaks, extreme temperatures).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    edf : pandas.DataFrame\n",
    "        A DataFrame containing:\n",
    "        - 'sorted': The observed values sorted in descending order.\n",
    "        - 'ranks': The rank of each value\n",
    "        - 'exceedance': The empirical exceedance probability (rank / (N + 1)).\n",
    "        - 'period': The empirical return period (1 / exceedance).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function assumes that the input data represents independent extreme events.\n",
    "    - The return period is an estimate based on the empirical data and does not assume a fitted\n",
    "      probability distribution.\n",
    "    \"\"\"\n",
    "    edf = pd.DataFrame(index=np.arange(1, len(sample)+1))\n",
    "    edf['sorted'] = sample.sort_values(ascending=False).values \n",
    "    edf['ranks'] = np.sort(stats.rankdata(-edf['sorted'])) \n",
    "    edf[\"exceedance\"] = edf[\"ranks\"] / (len(edf) + 1)\n",
    "    edf[\"period\"] = 1 / edf[\"exceedance\"]\n",
    "    return edf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "No need to learn the details of python functions if you don't want to - the documentation should hopefully help. Now it's simple to compute the empirical return levels of any sample, say for example annual extreme daily precipitation at Chew valley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual daily maxima\n",
    "annual_max = cdr.resample('YS').max()\n",
    "\n",
    "# Compute the statistics\n",
    "edf = return_period(annual_max)\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(edf['period'], edf['sorted'], \"o\")\n",
    "ax.grid(True)\n",
    "ax.set_title('Fig. 1: return periods/levels of annual maximum daily precipitation at Chew')\n",
    "ax.set_xlabel(\"Return Period (years)\")\n",
    "ax.set_ylabel(\"Return Level (mm/day)\")\n",
    "ax.set_xscale(\"log\"); ax.set_yscale(\"log\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "The plot above is the same than last week. Now let's make an experiment. How would the return levels change if the most extreme event didn't happen and was replaced, say, with a \"normal\" annual precipitation maxima? Let's play the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual daily maxima - but altered. We replace the max with the median\n",
    "annual_max_fake = cdr.resample('YS').max()\n",
    "annual_max_fake.loc[annual_max_fake.idxmax()] = np.median(annual_max_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**E: Explore the content of `annual_max` and `annual_max_fake`. Can you spot the one difference? What is the new maximal value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Good. Now I'll make a new plot with the real data and the \"altered\" data series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistics\n",
    "edf_fake = return_period(annual_max_fake)\n",
    "\n",
    "# Plot them\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(edf['period'], edf['sorted'], \"o\", label='Empirical')\n",
    "ax.plot(edf['period'], edf_fake['sorted'], \"o\", label='\"Altered\"')\n",
    "ax.grid(True)\n",
    "ax.set_title('Fig. 2: Return periods/levels of annual maximum daily precipitation at Chew')\n",
    "ax.set_xlabel(\"Return Period (years)\")\n",
    "ax.set_ylabel(\"Return Level (mm/day)\")\n",
    "plt.legend();\n",
    "ax.set_xscale(\"log\"); ax.set_yscale(\"log\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**E: either by looking at the plot (with or without logarithmic scales) or by looking at the `edf` and `edf_fake` variables, adress the following questions:**\n",
    "- **what is the approximate return level of a 30-year precipitation event in each case (real or \"altered\")?**\n",
    "- **what is the approximate return period of a 55 mm/day precipitation event in each case (real or \"altered\")?**\n",
    "- **is there any visible impact for the \"lower\" extreme values, i.e. < 30 mm/day?**\n",
    "**Discuss the robustness of empirical return periods based on this simple example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Bonus: get a sense of empirical uncertainty with \"bootstrapping\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We have seen above that even a very small change in the data series can have large consequences for the computation of return levels and return periods, especially for very rare events. Intuitively, this makes sense: rare events are, by definition, rare and difficult to predict. Until they actually occur, we are prone to making large errors in estimating their probability of occurrence or their intensity.\n",
    "\n",
    "Another way to understand this uncertainty is by considering the 1968 flood event. It occurred after only 10 years of measurements, meaning that if we had performed this exercise at the time, its empirical return period would have been estimated at 10 years. Today, after 60 years of measurements, the empirical return period has increased to 60 years because no stronger event has occurred since. In short, we have likely still underestimated the true return period of this event, as we lack insights into the next, even larger event that may occur in the future.\n",
    "\n",
    "Beyond these qualitative considerations, there are more quantitative methods to explore the uncertainty arising from the fact that our records are always too short to fully capture extreme values—even when they span 60 years. A key technique that allows us to assess this uncertainty using the data itself is called bootstrapping, named after the saying “pulling oneself up by one’s bootstraps.” In essence, we will leverage our limited dataset to estimate the uncertainty that arises from its finite length.\n",
    "\n",
    "Quoting [climatematch](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Tutorial2.html):\n",
    "\n",
    "> This method works by assuming that while we have limited data, the data that we do have is nevertheless a representative sample of possible events - that is we assume that we did not miss anything systematic. Then, we use the data as a distribution to draw “fake” observations from, and compute uncertainties using those:\n",
    "> - We pull artificial observations from our data - with replacement.\n",
    "> - We draw as many observations as our data record is long. This record will now only include data points that we observed before, but some will show up several times, some not at all.\n",
    "> - We compute our statistics over this artifical record - just as we did to the record itself.\n",
    "> - We repeat this process many times.\n",
    ">\n",
    "> Thus we compute our statistics over many artificial records, which can give us an indication of the uncertainty.\n",
    "> Remember, this only works under the assumption that our data is representative of the process - that we did not systematically miss something about the data. We cannot use this to estimate measurements errors for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Bootstrapping is relatively easy to implement, but it requires the use of \"for\" loops. Learning for-loops is not part of this lecture, therefore let me do this for you now. I can only recommend you to try to understand the code below the best you can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup figure\n",
    "fix, ax = plt.subplots()\n",
    "\n",
    "# create 1000 resamples of the data\n",
    "for i in range(1000):\n",
    "\n",
    "    # Create a fake sample, with some years removed or pulled twice, but always of the same length\n",
    "    sample = annual_max.sample(n=len(annual_max), replace=True)\n",
    "    # Compute the return periods\n",
    "    edf_s = return_period(sample)\n",
    "    # Plot them as transparent black lines\n",
    "    ax.plot(edf_s['period'], edf_s['sorted'], color='k', alpha=0.5, linewidth=0.2)\n",
    "\n",
    "# Now plot the \"true\" ones\n",
    "edf = return_period(annual_max)\n",
    "ax.plot(edf['period'], edf['sorted'], \"o\")\n",
    "ax.grid(True)\n",
    "ax.set_title('Fig. 3: Return periods/levels of annual maximum daily precipitation at Chew')\n",
    "ax.set_xlabel(\"Return Period (years)\")\n",
    "ax.set_ylabel(\"Return Level (mm/day)\")\n",
    "ax.set_xscale(\"log\"); ax.set_yscale(\"log\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The plot above (Fig. 3) is a generalisation of \"Fig. 2\" further up. While the previous attempt was only one example, in the above we explored many other possible sequences of 60 years, where certain extreme events happened more, or less often than they happened in reality. \n",
    "\n",
    "**Q: by looking at the plot (possibly by trying with / without log axes), answer the following questions:**\n",
    "- **What is the full possible range of return periods for a 100 mm/day event, based on the bootstrapped samples?**\n",
    "- **What is the full possible range of return levels for a 50-year event, based on the bootstrapped samples?**\n",
    "- **Where is the spread of possible outcomes largest? Can you make sense of why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## The GEV distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "To address some of the limitations of empirical return values and to make predictions about return levels and return periods for events that haven’t occurred yet, risk analysts rely on a branch of statistics called [extreme value theory](https://en.wikipedia.org/wiki/Extreme_value_theory). We won’t cover all the details here — you’ll learn them if you need them for your dissertation or future job — but it’s still useful to have a basic understanding of how it works.\n",
    "\n",
    "Let’s start by recalling that probabilities of exceedance (the inverse of return periods) can be computed from theoretical distributions. Last week, we used a Gaussian distribution to calculate probabilities of exceedance. The approach we’ll take here follows the same principle, but with one key observation: while a Gaussian distribution provides a reasonable representation of daily temperatures, it does not adequately capture extreme values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(20, 130, 4)\n",
    "\n",
    "annual_max.plot.hist(edgecolor='k', bins=bins, density=True, label='Events'); \n",
    "plt.title('Histogram of annual max daily precipitation at Chew'); \n",
    "plt.xlabel('Daily precipitation (mm/day)');\n",
    "\n",
    "# add PDF\n",
    "x_pdf = np.arange(5, 130, 0.1)\n",
    "y_pdf = stats.norm.pdf(x_pdf, np.mean(annual_max), np.std(annual_max))\n",
    "plt.plot(x_pdf, y_pdf, c=\"k\", lw=3, label='Gaussian PDF');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "**Q: describe qualitatively how well (or how bad) the Gaussian distribution fits the data. Is there a tendency to underestimate the occurence of some values? Or overestimate others?**\n",
    "\n",
    "**Bonus excercise (optional): a good way to check if a distribution matches the data well is to generate a [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). This is not a statistics class - but if you are adventurous, visit [climatematch](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Tutorial3.html#section-1-precipitation-data-histogram-and-normal-distribution) for some context, and code your own below!**\n",
    "\n",
    "[Q-Q plot for the bonus exercise](https://raw.githubusercontent.com/fmaussion/climate_risks/refs/heads/main/book/img/qq_gaussian.png), & [code](https://github.com/fmaussion/climate_risks/blob/main/book/ws06/qqplot.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Fitting the GEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Okay, so we agree that the Gaussian distribution is not an accurate representation of our data. This is because the Gaussian distribution is determined by only two parameters (the mean and the standard deviation) and is symmetric around its central value. However, extreme value histograms often exhibit a “thick tail” formed by rare, extreme events. Fortunately, other distributions can better capture these characteristics. Let’s explore the [generalized extreme value](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution) (GEV) distribution.\n",
    "\n",
    "The GEV distribution introduces a third parameter. The location and scale parameters behave similarly to the mean and standard deviation in a normal distribution. However, the shape parameter impacts the tails of the distribution, making them either thinner or thicker. Since extreme event distributions often exhibit thick tails, they are typically better represented by the GEV distribution.\n",
    "\n",
    "To estimate the parameters of the GEV distribution, we use the [stats.genextreme()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.genextreme.html#scipy-stats-genextreme) function from the scipy package. The three GEV parameters (location, scale, and shape) can be estimated from data using the `gev.fit()` method. The second argument in `gev.fit()`, shown in the example below, is optional (it provides an initial guess for the shape parameter). In some cases, setting this to zero is beneficial, as the fitting algorithm can be unstable and may return incorrect values otherwise. (*Hint: Always check if your results make sense!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, loc, scale = stats.genextreme.fit(annual_max, 0)\n",
    "shape, loc, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Now let's see if the GEV fits our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(20, 130, 4)\n",
    "\n",
    "annual_max.plot.hist(edgecolor='k', bins=bins, density=True, label='Events'); \n",
    "plt.title('Histogram of annual max daily precipitation at Chew'); \n",
    "plt.xlabel('Daily precipitation (mm/day)');\n",
    "\n",
    "# add Gaussian PDF\n",
    "x_pdf = np.arange(5, 130, 0.1)\n",
    "y_pdf = stats.norm.pdf(x_pdf, np.mean(annual_max), np.std(annual_max))\n",
    "plt.plot(x_pdf, y_pdf, c=\"k\", lw=3, label='Gaussian PDF');\n",
    "\n",
    "# add GEV PDF\n",
    "x_pdf = np.arange(5, 130, 0.1)\n",
    "y_pdf = stats.genextreme.pdf(x_pdf, shape, loc=loc, scale=scale)\n",
    "plt.plot(x_pdf, y_pdf, c=\"C3\", lw=3, label='GEV PDF');\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "**Q: Describe (qualitatively) how the GEV matches the data. Now look closely at the more extreme values in the range - do you see an improvement here as well?**\n",
    "\n",
    "**E: in a separate graph, try various values for the shape, location and scale parameters, and see how it influences the distribution.** *Hint: you can get some inspiration from [climatematch](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Tutorial3.html#coding-exercise-1)*.\n",
    "\n",
    "**Bonus exercise: if you did the qq plot exercise above, repeat it here. Does it look better?**\\\n",
    "\n",
    "[Q-Q plot for the bonus exercise](https://raw.githubusercontent.com/fmaussion/climate_risks/refs/heads/main/book/img/qq_gev.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Computing return levels and return periods with the GEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Now we have a mathematical representation of the distribution of our extreme values. Unlike our previous computations on a discrete sample of values, we can now use SciPy to compute return levels and return periods for *any event* using a continuous distribution.\n",
    "\n",
    "- `stats.genextreme.cdf()` gives us the **cumulative distribution function (CDF)** for a given value.\n",
    "- `stats.genextreme.ppf()` provides the **inverse of the CDF** (also called the **percent point function, PPF**).\n",
    "\n",
    "These may sound like complex terms, but they represent simple concepts that are best understood through examples.\n",
    "\n",
    "Last week, we learned that the **probability of exceedance** is simply:\n",
    "\n",
    "$$\n",
    "P(X \\geq x) = 1 - F(x)\n",
    "$$\n",
    "\n",
    "where $F(x)$ is the **CDF**.\n",
    "\n",
    "For example, to compute the probability of exceedance for a return level of **50 mm/day**, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 50  # chosen return level in mm/day\n",
    "p = 1 - stats.genextreme.cdf(v, shape, loc=loc, scale=scale)  # Between 0 and 1. Multiply by 100 for %\n",
    "f'The probability of exceedance for a return level of {v} mm/day is {p*100:.1f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "**E: repeat the computation for other return levels, for example 100, 120, 150.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Once again, the probabilities become very small for very high return levels (hence very rare events). As a reminder, the **return period** is simply:\n",
    "\n",
    "$$\n",
    "T = \\frac{1}{P(X \\geq x)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $T$ is the **return period** (in years, if annual maxima are used),\n",
    "- $P(X \\geq x)$ is the **probability of exceedance**, i.e., the probability that an event of magnitude \\( x \\) or greater will occur.\n",
    "\n",
    "Since the **probability of exceedance** is given by $P(X \\geq x) = 1 - F(x)$ where $F(x)$ is the **cumulative distribution function (CDF)**, we can rewrite the return period as:\n",
    "\n",
    "$$\n",
    "T = \\frac{1}{1 - F(x)}\n",
    "$$\n",
    "\n",
    "Or, in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 50  # chosen return level in mm/day\n",
    "p = 1 - stats.genextreme.cdf(v, shape, loc=loc, scale=scale) \n",
    "t = 1 / p\n",
    "f'The return period for a return level of {v} mm/day is {t:.0f} years'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "**E: what is the return period for the 1968 event of 122.2 mm/day, based on our fitted GEV? How does it compare with the empirical return period we computed earlier? Which do you think is more plausible?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "We have now learned how to compute **return periods** for a given level. But what about the inverse, i.e. **computing a return level** from a chosen return period?\n",
    "\n",
    "This is where the **percent point function (PPF)** comes into play. The PPF is the inverse of the **cumulative distribution function (CDF)** and allows us to determine the return value $x$ associated with a given probability.\n",
    "\n",
    "Since the **return period** is defined as:\n",
    "\n",
    "$$\n",
    "T = \\frac{1}{1 - F(x)}\n",
    "$$\n",
    "\n",
    "we can solve for $x$ by inverting the CDF:\n",
    "\n",
    "$$\n",
    "x = F^{-1}(1 - \\frac{1}{T})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the **return level** for a given return period $T$,\n",
    "- $F^{-1}$ is the **inverse CDF** (also called the PPF),\n",
    "- $T$ is the **return period** (e.g., 100 years for a 100-year event).\n",
    "\n",
    "Or, in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100  # chosen return period in years\n",
    "v = stats.genextreme.ppf(1 - 1/t, shape, loc=loc, scale=scale)  # Return value\n",
    "f'The return value for an {t}-year event is {v:.1f} mm/day'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "**E: check that the return period for an event of 122.2 mm/day is what you expect from your calculations above. Try a few other return periods as you see fit. What is the return level of a 1000-year event?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "**E: by noting that in all the simple code snippets above, `stats.genextreme` can be replaced by `stats.norm` (for the gaussian distribution), compute the return period of a 122.2 mm/day event with a gaussian distribution. What do you think about this value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### The GEV return level / return values plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Let's summarize all the computations above in a single plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Empirical  statistics\n",
    "edf = return_period(annual_max)\n",
    "ax.plot(edf['period'], edf['sorted'], \"o\", label='Empirical')\n",
    "\n",
    "# create array of years\n",
    "years = np.arange(1.1, 400, 0.1)\n",
    "\n",
    "# GEV return values for these years\n",
    "ax.plot(years, stats.genextreme.ppf(1 - 1 / years, shape, loc=loc, scale=scale), lw=2, c='k', label='GEV')\n",
    "\n",
    "# Gaussian return values for these years\n",
    "ax.plot(years, stats.norm.ppf(1 - 1 / years, annual_max.mean(), annual_max.std()), c='C1', label='Gaussian')\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_title('Return periods/levels of annual maximum daily precipitation at Chew')\n",
    "ax.set_xlabel(\"Return Period (years)\")\n",
    "ax.set_ylabel(\"Return Level (mm/day)\")\n",
    "ax.set_xscale(\"log\"); ax.set_yscale(\"log\"); \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Remember that the purpose of the GEV is to address some of the issues posed by empirical return levels (as explained above) and by the use of gaussian distribution for highly non-normal extreme values distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
